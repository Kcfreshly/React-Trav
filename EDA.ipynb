{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de2c6c3d-ea04-46fa-b4c1-16e1ab298fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "import shap\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder,FunctionTransformer, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from scipy.stats import chi2_contingency, ttest_ind \n",
    "import optuna\n",
    "\n",
    "\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import pandas as pd\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "# from sklearn.preprocessing import StandardScaler  # only if you uncomment it\n",
    "\n",
    "# target encoding\n",
    "import category_encoders as ce  # pip install category-encoders\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6102c4d6-f1b9-4af0-ab46-ab242633dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tables(data_path=\"data/\"):\n",
    "    \"\"\"Simple function to load all Home Credit tables\"\"\"\n",
    "    \n",
    "    tables = {}\n",
    "    \n",
    "    # Load each file with encoding handling\n",
    "    files = ['application_train.csv', 'bureau.csv', 'bureau_balance.csv', \n",
    "             'previous_application.csv', 'installments_payments.csv',\n",
    "             'credit_card_balance.csv', 'POS_CASH_balance.csv']\n",
    "    \n",
    "    for filename in files:\n",
    "        table_name = filename.replace('.csv', '')\n",
    "        try:\n",
    "            # Try common encodings\n",
    "             tables[table_name] = pd.read_csv(f\"{data_path}/{filename}\", encoding=encoding)\n",
    "        except:\n",
    "            tables[table_name] = pd.read_csv(f\"{data_path}/{filename}\", encoding='windows-1252')\n",
    "    \n",
    "    return tables\n",
    "\n",
    "tables = load_tables(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2faf5f-bba8-44bc-b4f7-69e987f98376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class HomeCreditFeatureEngineer:\n",
    "    def __init__(self, tables: dict):\n",
    "        self.tables = tables\n",
    "        self.features = {}\n",
    "\n",
    "    # -------------------------\n",
    "    # Application Features\n",
    "    # -------------------------\n",
    "    def create_application_features(self):\n",
    "        df = self.tables['application_train'].copy()\n",
    "        print('Creating application features...')\n",
    "\n",
    "        # Credit/income ratios\n",
    "        df['CREDIT_INCOME_PERCENT']  = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 1e-8)\n",
    "        df['ANNUITY_INCOME_PERCENT'] = df['AMT_ANNUITY'] / (df['AMT_INCOME_TOTAL'] + 1e-8)\n",
    "        df['CREDIT_TERM']            = df['AMT_ANNUITY'] / (df['AMT_CREDIT'] + 1e-8)\n",
    "        df['GOODS_PRICE_CREDIT_PCT'] = df['AMT_GOODS_PRICE'] / (df['AMT_CREDIT'] + 1e-8)\n",
    "\n",
    "        # Age/employment\n",
    "        df['DAYS_BIRTH'] = df['DAYS_BIRTH'].abs()\n",
    "        df['AGE_YEARS']  = df['DAYS_BIRTH'] / 365.25\n",
    "        df['DAYS_EMPLOYED_PERCENT'] = df['DAYS_EMPLOYED'] / (df['DAYS_BIRTH'] + 1e-8)\n",
    "        df['INCOME_PER_PERSON']     = df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1e-8)\n",
    "        df['CHILDREN_RATIO']        = df['CNT_CHILDREN'] / (df['CNT_FAM_MEMBERS'] + 1e-8)\n",
    "\n",
    "        # External sources\n",
    "        ext = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']\n",
    "        df['EXT_SOURCES_MEAN'] = df[ext].mean(axis=1, skipna=True)\n",
    "        df['EXT_SOURCES_STD']  = df[ext].std(axis=1,  skipna=True)\n",
    "\n",
    "        # Documents\n",
    "        doc_cols = [c for c in df.columns if 'FLAG_DOCUMENT' in c]\n",
    "        df['DOCUMENT_COUNT'] = df[doc_cols].sum(axis=1)\n",
    "\n",
    "        # Contact info\n",
    "        df['PHONE_TO_BIRTH_RATIO'] = df['DAYS_LAST_PHONE_CHANGE'] / (df['DAYS_BIRTH'] + 1e-8)\n",
    "\n",
    "        self.features['application'] = df\n",
    "        print(f\"Application features created: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    # -------------------------\n",
    "    # Bureau + Bureau Balance\n",
    "    # -------------------------\n",
    "    def create_bureau_features(self):\n",
    "        bureau     = self.tables['bureau'].copy()\n",
    "        bureau_bal = self.tables['bureau_balance'].copy()\n",
    "        print(\"Creating bureau features...\")\n",
    "\n",
    "        # Map STATUS to numeric delinquency\n",
    "        status_map = {'C':0,'X':0,'0':0,'1':1,'2':2,'3':3,'4':4,'5':5}\n",
    "        bureau_bal['STATUS_NUM'] = bureau_bal['STATUS'].map(status_map).fillna(0)\n",
    "\n",
    "        bb_agg = bureau_bal.groupby('SK_ID_BUREAU').agg(\n",
    "            MONTHS_BALANCE_MIN=('MONTHS_BALANCE','min'),\n",
    "            MONTHS_BALANCE_MAX=('MONTHS_BALANCE','max'),\n",
    "            MONTHS_BALANCE_SIZE=('MONTHS_BALANCE','count'),\n",
    "            STATUS_C_COUNT=('STATUS', lambda s: (s=='C').sum()),\n",
    "            STATUS_BAD_COUNT=('STATUS_NUM', lambda s: (s>=1).sum()),\n",
    "            MAX_DELQ=('STATUS_NUM','max'),\n",
    "            MEAN_DELQ=('STATUS_NUM','mean')\n",
    "        ).reset_index()\n",
    "\n",
    "        bureau = bureau.merge(bb_agg, on='SK_ID_BUREAU', how='left')\n",
    "\n",
    "        # Credit utilization\n",
    "        bureau['CREDIT_UTILIZATION'] = bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM_LIMIT'] + 1e-8)\n",
    "\n",
    "        # Aggregations per client\n",
    "        num_aggs = {\n",
    "            'DAYS_CREDIT':['mean','min','max'],\n",
    "            'CREDIT_DAY_OVERDUE':['sum','max'],\n",
    "            'DAYS_CREDIT_ENDDATE':['mean','min','max'],\n",
    "            'DAYS_ENDDATE_FACT':['mean','min','max'],\n",
    "            'AMT_CREDIT_MAX_OVERDUE':['mean','max'],\n",
    "            'CNT_CREDIT_PROLONG':['sum','mean'],\n",
    "            'AMT_CREDIT_SUM':['sum','mean','max'],\n",
    "            'AMT_CREDIT_SUM_DEBT':['sum','mean'],\n",
    "            'AMT_CREDIT_SUM_LIMIT':['sum','mean','max'],\n",
    "            'DAYS_CREDIT_UPDATE':['mean','max'],\n",
    "            'AMT_ANNUITY':['mean','sum'],\n",
    "            'CREDIT_UTILIZATION':['mean','max'],\n",
    "            'MAX_DELQ':['max'],\n",
    "            'MEAN_DELQ':['mean']\n",
    "        }\n",
    "        b_num = bureau.groupby('SK_ID_CURR').agg(num_aggs)\n",
    "        b_num.columns = ['BUREAU__' + '_'.join(c).upper() for c in b_num.columns.to_flat_index()]\n",
    "\n",
    "        b_cat = bureau.groupby('SK_ID_CURR').agg(\n",
    "            CREDIT_ACTIVE_COUNT=('CREDIT_ACTIVE','count'),\n",
    "            CREDIT_ACTIVE_ACTIVE=('CREDIT_ACTIVE', lambda s: (s=='Active').sum()),\n",
    "            CREDIT_TYPE_NUNIQUE=('CREDIT_TYPE','nunique')\n",
    "        )\n",
    "\n",
    "        bureau_df = b_num.join(b_cat, how='left').reset_index()\n",
    "\n",
    "        # Recent 2 years\n",
    "        recent = bureau[bureau['DAYS_CREDIT'] >= -730]\n",
    "        if not recent.empty:\n",
    "            recent_agg = recent.groupby('SK_ID_CURR').agg(\n",
    "                RECENT_CREDIT_COUNT=('SK_ID_BUREAU','count'),\n",
    "                RECENT_CREDIT_SUM=('AMT_CREDIT_SUM','sum')\n",
    "            )\n",
    "            bureau_df = bureau_df.merge(recent_agg.reset_index(), on='SK_ID_CURR', how='left')\n",
    "\n",
    "        self.features['bureau'] = bureau_df\n",
    "        print(f\"Bureau features created: {bureau_df.shape}\")\n",
    "        return bureau_df\n",
    "\n",
    "    # -------------------------\n",
    "    # Previous Applications\n",
    "    # -------------------------\n",
    "    def create_previous_application_features(self):\n",
    "        prev = self.tables['previous_application'].copy()\n",
    "        print(\"Creating previous application features...\")\n",
    "\n",
    "        prev['APP_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / (prev['AMT_CREDIT'] + 1e-8)\n",
    "\n",
    "        num_aggs = {\n",
    "            'AMT_ANNUITY':['mean','max','min'],\n",
    "            'AMT_APPLICATION':['mean','max','min'],\n",
    "            'AMT_CREDIT':['mean','max','min'],\n",
    "            'AMT_DOWN_PAYMENT':['mean','max','min'],\n",
    "            'AMT_GOODS_PRICE':['mean','max','min'],\n",
    "            'HOUR_APPR_PROCESS_START':['mean','max','min'],\n",
    "            'RATE_DOWN_PAYMENT':['mean','max','min'],\n",
    "            'DAYS_DECISION':['mean','max'],\n",
    "            'CNT_PAYMENT':['mean','max','min'],\n",
    "            'APP_CREDIT_RATIO':['mean','max','min']\n",
    "        }\n",
    "        p_num = prev.groupby('SK_ID_CURR').agg(num_aggs)\n",
    "        p_num.columns = ['PREV__' + '_'.join(c).upper() for c in p_num.columns.to_flat_index()]\n",
    "\n",
    "        p_cat = prev.groupby('SK_ID_CURR').agg(\n",
    "            PREV_APP_COUNT=('SK_ID_PREV','count'),\n",
    "            PREV_APP_APPROVED=('NAME_CONTRACT_STATUS', lambda s: (s=='Approved').sum()),\n",
    "            PREV_APP_REFUSED=('NAME_CONTRACT_STATUS', lambda s: (s=='Refused').sum())\n",
    "        )\n",
    "\n",
    "        prev_df = p_num.join(p_cat, how='left').reset_index()\n",
    "        self.features['previous'] = prev_df\n",
    "        print(f\"Previous application features created: {prev_df.shape}\")\n",
    "        return prev_df\n",
    "\n",
    "    # -------------------------\n",
    "    # Installments\n",
    "    # -------------------------\n",
    "    def create_installments_features(self):\n",
    "        ins = self.tables['installments_payments'].copy()\n",
    "        print(\"Creating installment features...\")\n",
    "\n",
    "        ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / (ins['AMT_INSTALMENT'] + 1e-8)\n",
    "        ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "        ins['DPD'] = (ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']).clip(lower=0)\n",
    "        ins['LATE_PAYMENT'] = (ins['DPD'] > 0).astype(int)\n",
    "\n",
    "        ins_aggs = {\n",
    "            'PAYMENT_PERC':['mean','std'],\n",
    "            'PAYMENT_DIFF':['mean'],\n",
    "            'DPD':['mean','max','var'],\n",
    "            'AMT_PAYMENT':['mean','max'],\n",
    "            'SK_ID_PREV':['count']\n",
    "        }\n",
    "        i_num = ins.groupby('SK_ID_CURR').agg(ins_aggs)\n",
    "        i_num.columns = ['INSTALL__' + '_'.join(c).upper() for c in i_num.columns.to_flat_index()]\n",
    "\n",
    "        i_cat = ins.groupby('SK_ID_CURR').agg(\n",
    "            INSTALL_LATE_PAYMENT_COUNT=('LATE_PAYMENT','sum'),\n",
    "            INSTALL_LATE_PAYMENT_RATE=('LATE_PAYMENT','mean')\n",
    "        )\n",
    "\n",
    "        ins_df = i_num.join(i_cat, how='left').reset_index()\n",
    "        self.features['installments'] = ins_df\n",
    "        print(f\"Installments features created: {ins_df.shape}\")\n",
    "        return ins_df\n",
    "\n",
    "    # -------------------------\n",
    "    # Credit Card\n",
    "    # -------------------------\n",
    "    def create_credit_card_features(self):\n",
    "        cc = self.tables['credit_card_balance'].copy()\n",
    "        print(\"Creating credit card features...\")\n",
    "\n",
    "        cc['BALANCE_LIMIT_RATIO'] = cc['AMT_BALANCE'] / (cc['AMT_CREDIT_LIMIT_ACTUAL'] + 1e-8)\n",
    "        cc['PAYMENT_MIN_RATIO']   = cc['AMT_PAYMENT_CURRENT'] / (cc['AMT_INST_MIN_REGULARITY'] + 1e-8)\n",
    "        cc['LATE_PAYMENT']        = (cc['SK_DPD'] > 0).astype(int)\n",
    "\n",
    "        cc_aggs = {\n",
    "            'AMT_BALANCE':['mean','max'],\n",
    "            'AMT_CREDIT_LIMIT_ACTUAL':['mean'],\n",
    "            'BALANCE_LIMIT_RATIO':['mean'],\n",
    "            'PAYMENT_MIN_RATIO':['mean'],\n",
    "            'SK_DPD':['mean','max'],\n",
    "            'SK_ID_PREV':['nunique']\n",
    "        }\n",
    "        c_num = cc.groupby('SK_ID_CURR').agg(cc_aggs)\n",
    "        c_num.columns = ['CC__' + '_'.join(c).upper() for c in c_num.columns.to_flat_index()]\n",
    "\n",
    "        c_cat = cc.groupby('SK_ID_CURR').agg(\n",
    "            CC_LATE_PAYMENT_COUNT=('LATE_PAYMENT','sum')\n",
    "        )\n",
    "\n",
    "        cc_df = c_num.join(c_cat, how='left').reset_index()\n",
    "        self.features['credit_card'] = cc_df\n",
    "        print(f\"Credit card features created: {cc_df.shape}\")\n",
    "        return cc_df\n",
    "\n",
    "    # -------------------------\n",
    "    # POS / Cash\n",
    "    # -------------------------\n",
    "    def create_pos_cash_features(self):\n",
    "        pos = self.tables['POS_CASH_balance'].copy()\n",
    "        print(\"Creating POS/Cash features...\")\n",
    "\n",
    "        pos['LATE_PAYMENT'] = (pos['SK_DPD'] > 0).astype(int)\n",
    "\n",
    "        pos_aggs = {\n",
    "            'CNT_INSTALMENT':['mean','max'],\n",
    "            'SK_DPD':['mean','max'],\n",
    "            'SK_ID_PREV':['nunique']\n",
    "        }\n",
    "        p_num = pos.groupby('SK_ID_CURR').agg(pos_aggs)\n",
    "        p_num.columns = ['POS__' + '_'.join(c).upper() for c in p_num.columns.to_flat_index()]\n",
    "\n",
    "        p_cat = pos.groupby('SK_ID_CURR').agg(\n",
    "            POS_LATE_PAYMENT_COUNT=('LATE_PAYMENT','sum')\n",
    "        )\n",
    "\n",
    "        pos_df = p_num.join(p_cat, how='left').reset_index()\n",
    "        self.features['pos_cash'] = pos_df\n",
    "        print(f\"POS/Cash features created: {pos_df.shape}\")\n",
    "        return pos_df\n",
    "\n",
    "    # -------------------------\n",
    "    # Merge All\n",
    "    # -------------------------\n",
    "    def create_all_features(self):\n",
    "        app_features  = self.create_application_features()\n",
    "        bureau_feats  = self.create_bureau_features()\n",
    "        prev_feats    = self.create_previous_application_features()\n",
    "        ins_feats     = self.create_installments_features()\n",
    "        cc_feats      = self.create_credit_card_features()\n",
    "        pos_feats     = self.create_pos_cash_features()\n",
    "\n",
    "        final_df = app_features.copy()\n",
    "        print(\"Merging all features...\")\n",
    "\n",
    "        for name, f in [\n",
    "            ('bureau', bureau_feats),\n",
    "            ('previous', prev_feats),\n",
    "            ('installments', ins_feats),\n",
    "            ('credit_card', cc_feats),\n",
    "            ('pos_cash', pos_feats)\n",
    "        ]:\n",
    "            final_df = final_df.merge(f, on='SK_ID_CURR', how='left')\n",
    "            print(f\"After {name}: {final_df.shape}\")\n",
    "\n",
    "        self.features['final'] = final_df\n",
    "        print(f\"Final feature set: {final_df.shape}\")\n",
    "        return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64f8f1d9-63a3-437d-a26e-039f8e0e6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Feature Engineering Class\n",
    "class AdvancedFeatureEngineer:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def create_polynomial_features(self, feature_pairs, degree=2):\n",
    "        \"\"\"Create polynomial combinations of important features\"\"\"\n",
    "        for feat1, feat2 in feature_pairs:\n",
    "            if feat1 in self.df.columns and feat2 in self.df.columns:\n",
    "                self.df[f'{feat1}_{feat2}_POLY'] = self.df[feat1] * self.df[feat2]\n",
    "    \n",
    "    def create_ratio_features(self, feature_pairs):\n",
    "        \"\"\"Create ratio features from feature pairs\"\"\"\n",
    "        for feat1, feat2 in feature_pairs:\n",
    "            if feat1 in self.df.columns and feat2 in self.df.columns:\n",
    "                self.df[f'{feat1}_{feat2}_RATIO'] = self.df[feat1] / (self.df[feat2] + 1e-8)\n",
    "    \n",
    "    def create_rank_features(self, features):\n",
    "        \"\"\"Create percentile rank features\"\"\"\n",
    "        for feat in features:\n",
    "            if feat in self.df.columns:\n",
    "                self.df[f'{feat}_RANK'] = self.df[feat].rank(pct=True)\n",
    "    \n",
    "    def apply_advanced_engineering(self):\n",
    "        \"\"\"Apply all advanced feature engineering\"\"\"\n",
    "        \n",
    "        # Important feature pairs for interactions\n",
    "        important_pairs = [\n",
    "            ('AMT_CREDIT', 'AMT_INCOME_TOTAL'),\n",
    "            ('AMT_ANNUITY', 'AMT_INCOME_TOTAL'),\n",
    "            ('EXT_SOURCE_1', 'EXT_SOURCE_2'),\n",
    "            ('EXT_SOURCE_2', 'EXT_SOURCE_3'),\n",
    "            ('BUREAU_AMT_CREDIT_SUM_MEAN', 'AMT_INCOME_TOTAL'),\n",
    "            ('CC_BALANCE_MEAN', 'AMT_INCOME_TOTAL')\n",
    "        ]\n",
    "        \n",
    "        self.create_polynomial_features(important_pairs)\n",
    "        self.create_ratio_features(important_pairs)\n",
    "        \n",
    "        # Rank features for important variables\n",
    "        rank_features = ['AMT_CREDIT', 'AMT_INCOME_TOTAL', 'EXT_SOURCES_MEAN', \n",
    "                        'BUREAU_AMT_CREDIT_SUM_MEAN', 'CREDIT_INCOME_PERCENT']\n",
    "        self.create_rank_features(rank_features)\n",
    "        \n",
    "        return self.df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ba0910c-b8e7-4291-8cbf-96b05fbee61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from heapq import nlargest\n",
    "from itertools import combinations\n",
    "\n",
    "class AutoFeatureEngineerTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Memory-safe AutoFE:\n",
    "      - Preselect top-K numeric base features by MI (or variance if y is None).\n",
    "      - Generate pairwise products & ratios one-by-one (streamed), no big dense matrix.\n",
    "      - Keep top-M by MI (or variance) for each type.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 cols=None,                 # optional numeric columns to consider\n",
    "                 base_top_k=30,             # how many base numeric cols to keep before crossing\n",
    "                 max_poly_features=60,      # how many product terms to keep\n",
    "                 max_ratio_features=60,     # how many ratio terms to keep\n",
    "                 use_mi=True,               # use mutual info with y to rank (else variance)\n",
    "                 random_state=42,\n",
    "                 dtype='float32'):\n",
    "        self.cols = cols\n",
    "        self.base_top_k = base_top_k\n",
    "        self.max_poly_features = max_poly_features\n",
    "        self.max_ratio_features = max_ratio_features\n",
    "        self.use_mi = use_mi\n",
    "        self.random_state = random_state\n",
    "        self.dtype = dtype\n",
    "\n",
    "        # learned\n",
    "        self.num_cols_ = None\n",
    "        self.base_cols_ = None\n",
    "        self.medians_ = None\n",
    "        self.keep_poly_pairs_ = []   # list of (f1, f2)\n",
    "        self.keep_ratio_pairs_ = []  # list of (f1, f2)\n",
    "\n",
    "    def _numeric_cols(self, X):\n",
    "        if self.cols is None:\n",
    "            return X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        return [c for c in self.cols if c in X.columns]\n",
    "\n",
    "    def _score_series(self, s, y):\n",
    "        # s: 1D series (float), y may be None\n",
    "        if self.use_mi and (y is not None):\n",
    "            # mutual_info_classif expects 2D\n",
    "            return float(mutual_info_classif(s.fillna(0).to_frame(), y,\n",
    "                                             discrete_features=False,\n",
    "                                             random_state=self.random_state)[0])\n",
    "        else:\n",
    "            return float(np.nanvar(s))\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.copy()\n",
    "        self.num_cols_ = self._numeric_cols(X)\n",
    "        if len(self.num_cols_) < 2:\n",
    "            # nothing to do\n",
    "            self.base_cols_ = self.num_cols_\n",
    "            self.medians_ = X[self.num_cols_].median() if self.num_cols_ else pd.Series(dtype=float)\n",
    "            return self\n",
    "\n",
    "        # medians for NaN handling\n",
    "        self.medians_ = X[self.num_cols_].median()\n",
    "\n",
    "        # --- preselect base numeric cols (top-K) ---\n",
    "        # score each numeric col\n",
    "        scores = {}\n",
    "        for c in self.num_cols_:\n",
    "            s = X[c].fillna(self.medians_[c]).astype(self.dtype)\n",
    "            scores[c] = self._score_series(s, y)\n",
    "        # pick top-K\n",
    "        self.base_cols_ = [c for c, _ in nlargest(self.base_top_k, scores.items(), key=lambda kv: kv[1])]\n",
    "\n",
    "        # --- stream products & ratios across base cols ---\n",
    "        poly_heap = []   # list of tuples (score, (f1, f2))\n",
    "        ratio_heap = []  # list of tuples (score, (f1, f2))\n",
    "\n",
    "        for f1, f2 in combinations(self.base_cols_, 2):\n",
    "            # product\n",
    "            s_prod = (X[f1].fillna(self.medians_[f1]).astype(self.dtype) *\n",
    "                      X[f2].fillna(self.medians_[f2]).astype(self.dtype))\n",
    "            sc_prod = self._score_series(s_prod, y)\n",
    "            poly_heap.append((sc_prod, (f1, f2)))\n",
    "\n",
    "            # ratio (f1 / f2)\n",
    "            s_ratio = X[f1].fillna(self.medians_[f1]).astype(self.dtype) / (\n",
    "                      X[f2].fillna(self.medians_[f2]).astype(self.dtype) + 1e-8)\n",
    "            sc_ratio = self._score_series(s_ratio, y)\n",
    "            ratio_heap.append((sc_ratio, (f1, f2)))\n",
    "\n",
    "        # keep top-M by score\n",
    "        self.keep_poly_pairs_  = [pair for _, pair in nlargest(self.max_poly_features,  poly_heap,  key=lambda t: t[0])]\n",
    "        self.keep_ratio_pairs_ = [pair for _, pair in nlargest(self.max_ratio_features, ratio_heap, key=lambda t: t[0])]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        if not self.base_cols_:\n",
    "            return X\n",
    "\n",
    "        # Recreate products\n",
    "        for f1, f2 in self.keep_poly_pairs_:\n",
    "            if f1 in X.columns and f2 in X.columns:\n",
    "                name = f\"{f1}__x__{f2}\"\n",
    "                X[name] = (X[f1].fillna(self.medians_.get(f1, 0)).astype(self.dtype) *\n",
    "                           X[f2].fillna(self.medians_.get(f2, 0)).astype(self.dtype))\n",
    "\n",
    "        # Recreate ratios\n",
    "        for f1, f2 in self.keep_ratio_pairs_:\n",
    "            if f1 in X.columns and f2 in X.columns:\n",
    "                name = f\"{f1}__div__{f2}\"\n",
    "                X[name] = X[f1].fillna(self.medians_.get(f1, 0)).astype(self.dtype) / (\n",
    "                          X[f2].fillna(self.medians_.get(f2, 0)).astype(self.dtype) + 1e-8)\n",
    "\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1192410-8a77-449d-8041-025af4c08c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HomeCreditPipeline:\n",
    "    def __init__(self, tables: dict):\n",
    "        self.tables = tables\n",
    "\n",
    "    def fit_transform(self):\n",
    "        # Base features\n",
    "        base_fe = HomeCreditFeatureEngineer(self.tables)\n",
    "        base_df = base_fe.create_all_features()\n",
    "\n",
    "        # Advanced features\n",
    "        adv_fe = AdvancedFeatureEngineer(base_df)\n",
    "        final_df = adv_fe.apply_advanced_engineering()\n",
    "\n",
    "        print(f\"Pipeline complete: {final_df.shape}\")\n",
    "        return final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762a53c5-83e0-45d9-b863-55c9d8821131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install scikit-learn category_encoders numpy pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# -----------------------------\n",
    "# Utility / Custom Transformers\n",
    "# -----------------------------\n",
    "\n",
    "class IdentityDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drop columns (e.g., IDs) from X; pass-thru others.\"\"\"\n",
    "    def __init__(self, drop_cols: List[str]):\n",
    "        self.drop_cols = drop_cols\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Assumes a DataFrame (used before ColumnTransformer)\n",
    "        cols = [c for c in X.columns if c not in self.drop_cols]\n",
    "        return X[cols]\n",
    "\n",
    "\n",
    "class CyclicalHourEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode an hour-of-day column (0-23) into sin/cos; drops the original.\"\"\"\n",
    "    def __init__(self, hour_col: str):\n",
    "        self.hour_col = hour_col\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Assumes a DataFrame (used before ColumnTransformer)\n",
    "        X = X.copy()\n",
    "        if self.hour_col not in X.columns:\n",
    "            return X\n",
    "        h = (X[self.hour_col].fillna(0).astype(float) % 24)\n",
    "        X[self.hour_col + \"_SIN\"] = np.sin(2 * np.pi * h / 24)\n",
    "        X[self.hour_col + \"_COS\"] = np.cos(2 * np.pi * h / 24)\n",
    "        X = X.drop(columns=[self.hour_col])\n",
    "        return X\n",
    "\n",
    "\n",
    "class QuantileClipper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Clip numeric columns to [q_low, q_high] percentiles to reduce outliers.\n",
    "    Works with both pandas DataFrames and NumPy ndarrays (ColumnTransformer safe).\n",
    "    \"\"\"\n",
    "    def __init__(self, q_low=0.01, q_high=0.99, cols: Optional[List[str]] = None):\n",
    "        self.q_low = q_low\n",
    "        self.q_high = q_high\n",
    "        self.cols = cols\n",
    "        # fitted state\n",
    "        self.bounds_ = None      # dict(col -> (lo, hi)) when DataFrame\n",
    "        self.lo_ = None          # ndarray quantile lows when ndarray\n",
    "        self.hi_ = None          # ndarray quantile highs when ndarray\n",
    "        self._df_mode = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self._df_mode = True\n",
    "            use_cols = self.cols or X.columns.tolist()\n",
    "            self.bounds_ = {}\n",
    "            for c in use_cols:\n",
    "                s = pd.to_numeric(X[c], errors='coerce')\n",
    "                lo = s.quantile(self.q_low)\n",
    "                hi = s.quantile(self.q_high)\n",
    "                self.bounds_[c] = (lo, hi)\n",
    "            self.lo_, self.hi_ = None, None\n",
    "        else:\n",
    "            self._df_mode = False\n",
    "            X = np.asarray(X, dtype=float)\n",
    "            self.lo_ = np.nanquantile(X, self.q_low, axis=0).astype(np.float32, copy=False)\n",
    "            self.hi_ = np.nanquantile(X, self.q_high, axis=0).astype(np.float32, copy=False)\n",
    "            self.bounds_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._df_mode or isinstance(X, pd.DataFrame):\n",
    "            # DF path\n",
    "            X = X.copy()\n",
    "            for c, (lo, hi) in self.bounds_.items():\n",
    "                if c in X.columns:\n",
    "                    X[c] = pd.to_numeric(X[c], errors='coerce').clip(lower=lo, upper=hi)\n",
    "            return X\n",
    "        else:\n",
    "            # ndarray path\n",
    "            X = np.asarray(X, dtype=float)\n",
    "            return np.clip(X, self.lo_, self.hi_)\n",
    "\n",
    "\n",
    "class Log1pTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Apply np.log1p to specified numeric columns (DataFrame) or to ALL columns (ndarray).\n",
    "    Safely shifts columns if any values < -1 to keep inputs valid for log1p.\n",
    "    Column-wise shifts are learned in fit() and reused in transform().\n",
    "    \"\"\"\n",
    "    def __init__(self, cols: Optional[List[str]] = None):\n",
    "        self.cols = cols\n",
    "        # fitted state\n",
    "        self._df_mode = None\n",
    "        self.shifts_ = None      # dict(col -> shift) when DataFrame\n",
    "        self.arr_shift_ = None   # ndarray shifts when ndarray\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            self._df_mode = True\n",
    "            self.shifts_ = {}\n",
    "            cols = self.cols or X.columns.tolist()\n",
    "            for c in cols:\n",
    "                if c not in X.columns:\n",
    "                    continue\n",
    "                s = pd.to_numeric(X[c], errors='coerce')\n",
    "                min_val = np.nanmin(s.values) if s.size else 0.0\n",
    "                self.shifts_[c] = 0.0 if (np.isnan(min_val) or min_val >= -1.0) else (-1.0 - min_val + 1e-8)\n",
    "            self.arr_shift_ = None\n",
    "        else:\n",
    "            self._df_mode = False\n",
    "            X = np.asarray(X, dtype=float)\n",
    "            mins = np.nanmin(X, axis=0)\n",
    "            shift = np.where(mins >= -1.0, 0.0, (-1.0 - mins + 1e-8))\n",
    "            self.arr_shift_ = shift.astype(np.float32, copy=False)\n",
    "            self.shifts_ = None\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        if self._df_mode or isinstance(X, pd.DataFrame):\n",
    "            # DF path\n",
    "            X = X.copy()\n",
    "            cols = self.cols or X.columns.tolist()\n",
    "            for c in cols:\n",
    "                if c in X.columns:\n",
    "                    v = pd.to_numeric(X[c], errors='coerce').values\n",
    "                    shift = self.shifts_.get(c, 0.0)\n",
    "                    X[c] = np.log1p(v + shift)\n",
    "            return X\n",
    "        else:\n",
    "            # ndarray path\n",
    "            X = np.asarray(X, dtype=float)\n",
    "            return np.log1p(X + self.arr_shift_)\n",
    "\n",
    "\n",
    "class BinaryCleaner(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Convert common Home Credit binary flags (0/1, 'Y'/'N', 'Yes'/'No', True/False) into 0/1.\n",
    "    Only applies to listed columns that exist. (Runs before ColumnTransformer.)\n",
    "    \"\"\"\n",
    "    def __init__(self, cols: List[str]):\n",
    "        self.cols = cols\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        # Assumes a DataFrame (used before ColumnTransformer)\n",
    "        X = X.copy()\n",
    "        for c in self.cols:\n",
    "            if c not in X.columns:\n",
    "                continue\n",
    "            s = X[c]\n",
    "            if s.dtype == 'bool':\n",
    "                X[c] = s.astype(int)\n",
    "            else:\n",
    "                X[c] = (\n",
    "                    s.replace({'Y': 1, 'N': 0, 'Yes': 1, 'No': 0, 'True': 1, 'False': 0})\n",
    "                     .astype(str).str.lower()\n",
    "                     .replace({'true': 1, 'false': 0, 'y': 1, 'n': 0})\n",
    "                )\n",
    "                X[c] = pd.to_numeric(X[c], errors='coerce')\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee13cf25-e85c-4bc7-91b0-37fdafc45780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Column detection helpers\n",
    "# -----------------------------\n",
    "\n",
    "def detect_column_groups(\n",
    "    X: pd.DataFrame,\n",
    "    known_ordinal: Dict[str, List[str]] = None,\n",
    "    known_binary: List[str] = None,\n",
    "    high_card_threshold: int = 12,\n",
    "    force_categorical_like: List[str] = None\n",
    ") -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Detect numeric vs categorical and split cat into low/high cardinality.\n",
    "    Also supports overrides for ordinal and \"categorical-looking numeric\" codes.\n",
    "    \"\"\"\n",
    "    known_ordinal = known_ordinal or {}\n",
    "    known_binary = known_binary or []\n",
    "    force_categorical_like = force_categorical_like or []  # e.g., ['SELLERPLACE_AREA']\n",
    "\n",
    "    id_cols = [c for c in X.columns if c in ('SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU')]\n",
    "    hour_cols = [c for c in X.columns if c == 'HOUR_APPR_PROCESS_START']\n",
    "\n",
    "    # Base detection\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    # Move forced categorical-like from numeric -> cat\n",
    "    for c in force_categorical_like:\n",
    "        if c in numeric_cols:\n",
    "            numeric_cols.remove(c)\n",
    "\n",
    "    # Categorical = objects + forced\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "    categorical_cols += [c for c in force_categorical_like if c in X.columns]\n",
    "\n",
    "    # Remove IDs and hour col from groups\n",
    "    for c in id_cols + hour_cols:\n",
    "        if c in numeric_cols: numeric_cols.remove(c)\n",
    "        if c in categorical_cols: \n",
    "            try: categorical_cols.remove(c)\n",
    "            except ValueError: pass\n",
    "\n",
    "    # Binary flags (override)\n",
    "    binary_cols = [c for c in known_binary if c in X.columns]\n",
    "\n",
    "    # Ordinal (override with provided order lists)\n",
    "    ordinal_cols = [c for c in known_ordinal.keys() if c in X.columns]\n",
    "\n",
    "    # Non-ordinal categorical (excluding binary & ordinal)\n",
    "    cat_nominal = [c for c in categorical_cols if c not in binary_cols + ordinal_cols]\n",
    "\n",
    "    # Split nominal into low/high cardinality\n",
    "    low_card_cats, high_card_cats = [], []\n",
    "    for c in cat_nominal:\n",
    "        nuni = X[c].nunique(dropna=True)\n",
    "        (low_card_cats if nuni <= high_card_threshold else high_card_cats).append(c)\n",
    "\n",
    "    return {\n",
    "        'ids': id_cols,\n",
    "        'hour': hour_cols,\n",
    "        'numeric': numeric_cols,\n",
    "        'binary': binary_cols,\n",
    "        'ordinal': ordinal_cols,\n",
    "        'ordinal_orders': known_ordinal,\n",
    "        'low_card_nominal': low_card_cats,\n",
    "        'high_card_nominal': high_card_cats\n",
    "    }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5eb26eb-3874-45a9-930f-b7b235f5e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Build the preprocessing pipeline\n",
    "# -----------------------------\n",
    "\n",
    "def build_homecredit_preprocessor(\n",
    "    X: pd.DataFrame,\n",
    "    *,\n",
    "    known_ordinal: Dict[str, List[str]] = None,\n",
    "    known_binary: List[str] = None,\n",
    "    force_categorical_like: List[str] = None,\n",
    "    high_card_threshold: int = 12,\n",
    "    log1p_cols: Optional[List[str]] = None,\n",
    "    clip_quantiles: Tuple[float,float] = (0.01, 0.99),\n",
    "    onehot_drop: str = 'if_binary',\n",
    "    target_smoothing: float = 0.2\n",
    ") -> Tuple[Pipeline, Dict[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Returns a sklearn Pipeline that:\n",
    "      - drops IDs,\n",
    "      - encodes hour cyclically,\n",
    "      - imputes + clips + log1p numerics,\n",
    "      - encodes binary, ordinal, low-card (OHE), high-card (TargetEncoder).\n",
    "    \"\"\"\n",
    "    log1p_cols = log1p_cols or []\n",
    "    col_groups = detect_column_groups(\n",
    "        X,\n",
    "        known_ordinal=known_ordinal,\n",
    "        known_binary=known_binary,\n",
    "        force_categorical_like=force_categorical_like,\n",
    "        high_card_threshold=high_card_threshold\n",
    "    )\n",
    "\n",
    "    # 1) First stage: column-level wrangling (drop IDs, cyclic hour, binary normalization)\n",
    "    initial_steps = []\n",
    "    if col_groups['ids']:\n",
    "        initial_steps.append((\"drop_ids\", IdentityDropper(drop_cols=col_groups['ids'])))\n",
    "    if col_groups['hour']:\n",
    "        # apply cyclical to the known hour column\n",
    "        initial_steps.append((\"hour_cyc\", CyclicalHourEncoder(hour_col=col_groups['hour'][0])))\n",
    "    if col_groups['binary']:\n",
    "        initial_steps.append((\"binary_clean\", BinaryCleaner(cols=col_groups['binary'])))\n",
    "\n",
    "    initial = Pipeline(initial_steps) if initial_steps else 'passthrough'\n",
    "\n",
    "    # 2) ColumnTransformer for typed processing\n",
    "    transformers = []\n",
    "\n",
    "    # Numeric pipeline\n",
    "    num_pipe = Pipeline(steps=[\n",
    "        ('impute', SimpleImputer(strategy='median')),\n",
    "        ('clip', QuantileClipper(q_low=clip_quantiles[0], q_high=clip_quantiles[1], cols=None)),\n",
    "        ('log1p', Log1pTransformer(cols=log1p_cols)),\n",
    "        # (Optional scaler can be added for linear/NN models)\n",
    "        # ('scaler', StandardScaler())\n",
    "    ])\n",
    "    if col_groups['numeric']:\n",
    "        transformers.append(('num', num_pipe, col_groups['numeric']))\n",
    "\n",
    "    # Binary (already normalized to 0/1) – impute if any missing\n",
    "    if col_groups['binary']:\n",
    "        bin_pipe = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "        ])\n",
    "        transformers.append(('bin', bin_pipe, col_groups['binary']))\n",
    "\n",
    "    # Ordinal pipeline (with explicit order)\n",
    "    if col_groups['ordinal']:\n",
    "        # Build categories in the required order for each ordinal col\n",
    "        categories = [col_groups['ordinal_orders'][c] for c in col_groups['ordinal']]\n",
    "        ord_pipe = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "            ('ord', OrdinalEncoder(categories=categories, handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "        ])\n",
    "        transformers.append(('ord', ord_pipe, col_groups['ordinal']))\n",
    "\n",
    "    # Low-card nominal → OneHot\n",
    "    if col_groups['low_card_nominal']:\n",
    "        ohe_pipe = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "            ('ohe', OneHotEncoder(handle_unknown='ignore', drop=onehot_drop, sparse_output=False))\n",
    "        ])\n",
    "        transformers.append(('ohe', ohe_pipe, col_groups['low_card_nominal']))\n",
    "\n",
    "    # High-card nominal → TargetEncoder (CV-safe when used inside sklearn CV)\n",
    "    if col_groups['high_card_nominal']:\n",
    "        te_pipe = Pipeline(steps=[\n",
    "            ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "            ('te', ce.TargetEncoder(smoothing=target_smoothing))\n",
    "        ])\n",
    "        transformers.append(('te', te_pipe, col_groups['high_card_nominal']))\n",
    "\n",
    "    ct = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop',\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "    # Final pipeline: initial wrangling → typed transformer\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('initial', initial),\n",
    "        ('ct', ct)\n",
    "    ])\n",
    "\n",
    "    return pipe, col_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155ae689-37c7-4743-890b-d81abf750d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) Manual FE from your tables\n",
    "# fe = HomeCreditPipeline(tables)         # your earlier class that merges & engineers\n",
    "# full_df = fe.fit_transform()            # one row per SK_ID_CURR\n",
    "# y = full_df['TARGET'].values\n",
    "# X = full_df.drop(columns=['TARGET'])\n",
    "\n",
    "# # 2) Build the preprocessing pipeline (from our previous step)\n",
    "# preprocessor, groups = build_homecredit_preprocessor(\n",
    "#     X,\n",
    "#     known_ordinal={'NAME_YIELD_GROUP': ['low_normal','low_action','middle','high','unknown']},\n",
    "#     known_binary=['FLAG_OWN_CAR','FLAG_OWN_REALTY','NFLAG_LAST_APPL_IN_DAY','NFLAG_INSURED_ON_APPROVAL'],\n",
    "#     force_categorical_like=['SELLERPLACE_AREA'],\n",
    "#     log1p_cols=['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','AMT_DOWN_PAYMENT']\n",
    "# )\n",
    "\n",
    "# # 3) Define AutoFE (let it auto-detect numeric cols)\n",
    "# auto_fe = AutoFeatureEngineerTransformer(\n",
    "#     cols=None,            # or pass a curated numeric list\n",
    "#     base_top_k=30,        # shrink to 30 best base numerics before crossing\n",
    "#     max_poly_features=60,\n",
    "#     max_ratio_features=60,\n",
    "#     use_mi=True,\n",
    "#     random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "# # 4) Build the master pipeline (AutoFE -> Preprocessor -> Model)\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from lightgbm import LGBMClassifier\n",
    "\n",
    "# master = Pipeline(steps=[\n",
    "#     ('auto_fe', auto_fe),\n",
    "#     ('prep', preprocessor),    # your ColumnTransformer with imputers/encoders\n",
    "#     ('clf', LGBMClassifier(\n",
    "#         n_estimators=2000, learning_rate=0.03,\n",
    "#         num_leaves=64, subsample=0.8, colsample_bytree=0.8,\n",
    "#         random_state=42\n",
    "#     ))\n",
    "# ])\n",
    "\n",
    "\n",
    "# # 5) CV / training (CV-safe: AutoFE and TargetEncoder see only train folds)\n",
    "# from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "# cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "# scores = cross_val_score(master, X, y, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "# print(\"CV AUC:\", scores.mean(), \"+/-\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "391ec0e1-3a92-472b-b08c-01581a219869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "class FiniteCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, max_abs=1e6, force_dense=True, dtype='float32'):\n",
    "        self.max_abs = max_abs\n",
    "        self.force_dense = force_dense\n",
    "        self.dtype = dtype\n",
    "    def fit(self, X, y=None): return self\n",
    "    def transform(self, X):\n",
    "        # to dense if requested\n",
    "        if sp.issparse(X):\n",
    "            X = X.toarray() if self.force_dense else X.tocsr()\n",
    "        X = np.asarray(X, dtype=self.dtype)\n",
    "        X[~np.isfinite(X)] = 0.0\n",
    "        np.clip(X, -self.max_abs, self.max_abs, out=X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "18e02d65-6818-4f22-be05-dbe46b65a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating application features...\n",
      "Application features created: (307511, 134)\n",
      "Creating bureau features...\n",
      "Bureau features created: (305811, 37)\n",
      "Creating previous application features...\n",
      "Previous application features created: (338857, 33)\n",
      "Creating installment features...\n",
      "Installments features created: (339587, 12)\n",
      "Creating credit card features...\n",
      "Credit card features created: (103558, 10)\n",
      "Creating POS/Cash features...\n",
      "POS/Cash features created: (337252, 7)\n",
      "Merging all features...\n",
      "After bureau: (307511, 170)\n",
      "After previous: (307511, 202)\n",
      "After installments: (307511, 213)\n",
      "After credit_card: (307511, 222)\n",
      "After pos_cash: (307511, 228)\n",
      "Final feature set: (307511, 228)\n",
      "Pipeline complete: (307511, 240)\n",
      "CV AUC: 0.7832860947013209 +/- 0.0028697116415542364\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "class HomeCreditPreprocessorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, builder, **builder_kwargs):\n",
    "        self.builder = builder                  # e.g., build_homecredit_preprocessor\n",
    "        self.builder_kwargs = builder_kwargs\n",
    "        self.ct_ = None\n",
    "        self.groups_ = None\n",
    "        self.feature_names_in_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
    "        self.feature_names_in_ = list(X_df.columns)\n",
    "        # Build the ColumnTransformer using the *current* columns (after AutoFE)\n",
    "        ct, groups = self.builder(X_df, **self.builder_kwargs)\n",
    "        self.ct_ = ct.fit(X_df, y)\n",
    "        self.groups_ = groups\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X, columns=self.feature_names_in_)\n",
    "        return self.ct_.transform(X_df)\n",
    "\n",
    "\n",
    "\n",
    "fe = HomeCreditPipeline(tables)\n",
    "full_df = fe.fit_transform()            # one row per SK_ID_CURR\n",
    "y = full_df['TARGET'].values\n",
    "X = full_df.drop(columns=['TARGET'])\n",
    "\n",
    "\n",
    "auto_fe = AutoFeatureEngineerTransformer(\n",
    "    cols=None,\n",
    "    base_top_k=30,\n",
    "    max_poly_features=60,\n",
    "    max_ratio_features=60,\n",
    "    use_mi=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "prep_tf = HomeCreditPreprocessorTransformer(\n",
    "    builder=build_homecredit_preprocessor,   # <-- pass the callable, don't call it here\n",
    "    known_ordinal={'NAME_YIELD_GROUP': ['low_normal','low_action','middle','high','unknown']},\n",
    "    known_binary=['FLAG_OWN_CAR','FLAG_OWN_REALTY','NFLAG_LAST_APPL_IN_DAY','NFLAG_INSURED_ON_APPROVAL'],\n",
    "    force_categorical_like=['SELLERPLACE_AREA'],\n",
    "    log1p_cols=['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','AMT_DOWN_PAYMENT']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "master = Pipeline(steps=[\n",
    "    ('auto_fe', auto_fe),\n",
    "    ('prep',    prep_tf),                     # now sees & processes AutoFE columns\n",
    "    ('clean',   FiniteCleaner(max_abs=1e6)),  # optional safety\n",
    "    ('clf',     LGBMClassifier(\n",
    "        n_estimators=2000, learning_rate=0.03,\n",
    "        num_leaves=64, subsample=0.8, colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(master, X, y, cv=cv, scoring='roc_auc', n_jobs=-1, error_score='raise')\n",
    "print(\"CV AUC:\", scores.mean(), \"+/-\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47cf9372-bca9-41db-9259-8409c6a750a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flaml lightgbm xgboost catboost --quiet\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from flaml import AutoML\n",
    "\n",
    "\n",
    "class FLAMLClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    sklearn-compatible classifier that uses FLAML under the hood.\n",
    "    Works inside sklearn Pipelines (so your AutoFE+Preprocessor are CV-safe).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 time_budget=1800,     # seconds\n",
    "                 metric=\"roc_auc\",\n",
    "                 eval_method=\"cv\",     # \"cv\" or \"holdout\"\n",
    "                 n_splits=5,\n",
    "                 task=\"classification\",\n",
    "                 verbose=1,\n",
    "                 seed=42,\n",
    "                 estimator_list=None,  # None = let FLAML choose freely\n",
    "                 **flaml_kwargs):\n",
    "        self.time_budget = time_budget\n",
    "        self.metric = metric\n",
    "        self.eval_method = eval_method\n",
    "        self.n_splits = n_splits\n",
    "        self.task = task\n",
    "        self.verbose = verbose\n",
    "        self.seed = seed\n",
    "        self.estimator_list = estimator_list\n",
    "        self.flaml_kwargs = flaml_kwargs\n",
    "        self.automl_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        automl = AutoML()\n",
    "        fit_kwargs = dict(\n",
    "            X_train=X,\n",
    "            y_train=y,\n",
    "            task=self.task,\n",
    "            time_budget=self.time_budget,\n",
    "            metric=self.metric,\n",
    "            eval_method=self.eval_method,\n",
    "            n_splits=self.n_splits if self.eval_method == \"cv\" else None,\n",
    "            verbose=self.verbose,\n",
    "            seed=self.seed,\n",
    "        )\n",
    "        if self.estimator_list is not None:\n",
    "            fit_kwargs[\"estimator_list\"] = self.estimator_list\n",
    "        # pass any extra FLAML args (e.g., constraints)\n",
    "        fit_kwargs.update(self.flaml_kwargs)\n",
    "\n",
    "        automl.fit(**fit_kwargs)\n",
    "        self.automl_ = automl\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.automl_ is None:\n",
    "            raise RuntimeError(\"Call fit() before predict_proba().\")\n",
    "        return self.automl_.predict_proba(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.automl_ is None:\n",
    "            raise RuntimeError(\"Call fit() before predict().\")\n",
    "        return self.automl_.predict(X)\n",
    "\n",
    "    @property\n",
    "    def best_estimator_(self):\n",
    "        return None if self.automl_ is None else self.automl_.best_estimator\n",
    "\n",
    "    @property\n",
    "    def best_config_(self):\n",
    "        return None if self.automl_ is None else self.automl_.best_config\n",
    "\n",
    "    @property\n",
    "    def best_model_(self):\n",
    "        return None if self.automl_ is None else self.automl_.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "465490bb-6f3f-42e7-b7e8-f560c0fae631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your existing instances (safe defaults for 16GB)\n",
    "auto_fe = AutoFeatureEngineerTransformer(\n",
    "    cols=None, base_top_k=16, max_poly_features=20, max_ratio_features=20,\n",
    "    use_mi=True, random_state=42, dtype='float32'\n",
    ")\n",
    "\n",
    "prep_tf = HomeCreditPreprocessorTransformer(\n",
    "    builder=build_homecredit_preprocessor,   # <-- pass the callable, don't call it here\n",
    "    known_ordinal={'NAME_YIELD_GROUP': ['low_normal','low_action','middle','high','unknown']},\n",
    "    known_binary=['FLAG_OWN_CAR','FLAG_OWN_REALTY','NFLAG_LAST_APPL_IN_DAY','NFLAG_INSURED_ON_APPROVAL'],\n",
    "    force_categorical_like=['SELLERPLACE_AREA'],\n",
    "    log1p_cols=['AMT_INCOME_TOTAL','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE','AMT_DOWN_PAYMENT']\n",
    ")\n",
    "\n",
    "fe_prep = Pipeline([\n",
    "    ('auto_fe', auto_fe),\n",
    "    ('prep',    prep_tf),\n",
    "    ('clean',   FiniteCleaner(max_abs=1e6, force_dense=True, dtype='float32')),\n",
    "])\n",
    "\n",
    "automl_clf = FLAMLClassifier(\n",
    "    time_budget=1800,\n",
    "    metric=\"roc_auc\",\n",
    "    eval_method=\"cv\",\n",
    "    n_splits=5,\n",
    "    verbose=2,\n",
    "    seed=42,\n",
    "    estimator_list=None\n",
    ")\n",
    "\n",
    "master_auto = Pipeline([\n",
    "    ('fe', fe_prep),\n",
    "    ('automl', automl_clf)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57c5ddb1-274e-4488-8118-ce55942986e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StratifiedKFold, cross_val_score\n\u001b[0;32m      3\u001b[0m cv \u001b[38;5;241m=\u001b[39m StratifiedKFold(n_splits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m scores \u001b[38;5;241m=\u001b[39m cross_val_score(estimator\u001b[38;5;241m=\u001b[39mmaster_auto, X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, cv\u001b[38;5;241m=\u001b[39mcv, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroc_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCV AUC:\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores\u001b[38;5;241m.\u001b[39mmean(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m+/-\u001b[39m\u001b[38;5;124m\"\u001b[39m, scores\u001b[38;5;241m.\u001b[39mstd())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:684\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    682\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 684\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m cross_validate(\n\u001b[0;32m    685\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m    686\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m    687\u001b[0m     y\u001b[38;5;241m=\u001b[39my,\n\u001b[0;32m    688\u001b[0m     groups\u001b[38;5;241m=\u001b[39mgroups,\n\u001b[0;32m    689\u001b[0m     scoring\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: scorer},\n\u001b[0;32m    690\u001b[0m     cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[0;32m    691\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[0;32m    692\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    693\u001b[0m     params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[0;32m    694\u001b[0m     pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch,\n\u001b[0;32m    695\u001b[0m     error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    696\u001b[0m )\n\u001b[0;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:411\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    410\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 411\u001b[0m results \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    412\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    413\u001b[0m         clone(estimator),\n\u001b[0;32m    414\u001b[0m         X,\n\u001b[0;32m    415\u001b[0m         y,\n\u001b[0;32m    416\u001b[0m         scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[0;32m    417\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[0;32m    418\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[0;32m    419\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m    420\u001b[0m         parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    421\u001b[0m         fit_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit,\n\u001b[0;32m    422\u001b[0m         score_params\u001b[38;5;241m=\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mscorer\u001b[38;5;241m.\u001b[39mscore,\n\u001b[0;32m    423\u001b[0m         return_train_score\u001b[38;5;241m=\u001b[39mreturn_train_score,\n\u001b[0;32m    424\u001b[0m         return_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    425\u001b[0m         return_estimator\u001b[38;5;241m=\u001b[39mreturn_estimator,\n\u001b[0;32m    426\u001b[0m         error_score\u001b[38;5;241m=\u001b[39merror_score,\n\u001b[0;32m    427\u001b[0m     )\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m indices\n\u001b[0;32m    429\u001b[0m )\n\u001b[0;32m    431\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:456\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Pipeline should either be a classifier to be used with response_method=predict_proba or the response_method should be 'predict'. Got a regressor with response_method=predict_proba instead."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(estimator=master_auto, X=X, y=y, cv=cv, scoring='roc_auc', n_jobs=-1, error_score='raise')\n",
    "print(\"CV AUC:\", scores.mean(), \"+/-\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2ec8d-6fa1-43ad-9dd1-b7d29be5f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(master, X, y, cv=cv, scoring='roc_auc',\n",
    "                         n_jobs=-1, error_score='raise')\n",
    "print(scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c07986-b951-4bb8-99ba-56266a175065",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
